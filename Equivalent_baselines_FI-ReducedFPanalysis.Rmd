---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.6.0
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

### Step-by-step computation of equivalent baselines

##### by Claudia

Given an array of horns, there is a certain number of baselines you can define. Afterwards, you can partitionate the set in subsets of equivalent baselines.

Let's work in a square array of $N=n \times n$ horns (for example, $n=8$ horns in a side of the array. Total number of horns: $N=64$)

```{python}
import numpy as np
import matplotlib.pyplot as plt
import glob
from itertools import combinations
import scipy.special
import qubic
from CSFPA_dataIO import calculate_intensity_4_baseline, IntegrateHornCombOnFP
import multiprocessing
from joblib import Parallel, delayed
from tqdm import tqdm
from timeit import default_timer as timer
import os
import re
from random import randint, sample

```

```{python}
n=20

N= n*n
```

```{python}
print("n= ",n, "N= ",N)
```

```{python}
"""try the same with alternate method using actual coordinates"""
"""above relates TD unique combinations
now setup FI numeric conversion"""
FIhorns = np.linspace(1,400,400, dtype=int)

d = qubic.qubicdict.qubicDict()
d.read_from_file('/home/james/libraries/qubic/qubic/dicts/pipeline_demo.dict')
d['config'] = 'FI'
q = qubic.QubicInstrument(d)

centers = q.horn.center[:, 0:2]
col = q.horn.column
row = q.horn.row

instFI = qubic.QubicInstrument(d)
hornsFI = instFI.horn.open

# hornsTD = (col >= 8) & (col <= 15) & (row >= 8) & (row <= 15)

# instTD = qubic.QubicInstrument(d)
# instTD.horn.open[~hornsTD] = False

cnt = 1
TDhornsFIconf = np.zeros(400)

# %matplotlib notebook
plt.figure(figsize=(10,10))
q.horn.plot()
for i in range(len(centers)):
    #if hornsTD[i] == True:
        #plt.text(centers[i,0]-0.006, centers[i,1], 'c{0:}'.format(col[i]), color='r',fontsize=8)
        #plt.text(centers[i,0]+0.00001, centers[i,1], 'r{0:}'.format(row[i]), color='b',fontsize=8)
    plt.text(centers[i,0]-0.005, centers[i,1], '{0:.1f}'.format(centers[i,0]*1000), color='r',fontsize=8)
    plt.text(centers[i,0]-0.005, centers[i,1]-0.004, '{0:.1f}'.format(centers[i,1]*1000), color='r',fontsize=8)
    #plt.text(centers[i,0]-0.005, centers[i,1]-0.004, 'h {0:}'.format(str(i+1)), color='g',fontsize=8)
    #plt.text(centers[i,0]-0.005, centers[i,1], 'td {0:}'.format(str(cnt)), color='r',fontsize=8)

    TDhornsFIconf[i] = cnt

    cnt+=1
#instTD.horn.plot()
plt.ylabel('Horn GRF Y (m)')
plt.xlabel('Horn GRF X (m)')

plt.show()


```

```{python}
plt.figure(figsize=(10,10))
q.horn.plot()
for i in range(len(centers)):
    #if hornsTD[i] == True:
        #plt.text(centers[i,0]-0.006, centers[i,1], 'c{0:}'.format(col[i]), color='r',fontsize=8)
        #plt.text(centers[i,0]+0.00001, centers[i,1], 'r{0:}'.format(row[i]), color='b',fontsize=8)
    plt.text(centers[i,0]-0.006, centers[i,1]-0.002, '{},{}'.format(q.horn.row[i], q.horn.column[i]), color='r',fontsize=8)
    #plt.text(centers[i,0]-0.005, centers[i,1]-0.004, '{0:.1f}'.format(centers[i,1]*1000), color='r',fontsize=8)
    #plt.text(centers[i,0]-0.005, centers[i,1]-0.004, 'h {0:}'.format(str(i+1)), color='g',fontsize=8)
    #plt.text(centers[i,0]-0.005, centers[i,1], 'td {0:}'.format(str(cnt)), color='r',fontsize=8)

    TDhornsFIconf[i] = cnt

    cnt+=1
#instTD.horn.plot()
plt.ylabel('Horn GRF Y (m)')
plt.xlabel('Horn GRF X (m)')

plt.show()
```

```{python}
# """thesis [plot]..."""
# font = {'family' : 'normal',
#         'weight' : 'normal',
#         'size'   : 22}

# plt.rc('font', **font)

# FIhorns = np.linspace(1,400,400, dtype=int)

# d = qubic.qubicdict.qubicDict()
# d.read_from_file('/home/james/libraries/qubic/qubic/dicts/pipeline_demo.dict')
# d['config'] = 'FI'
# q = qubic.QubicInstrument(d)

# centers = q.horn.center[:, 0:2]
# col = q.horn.column
# row = q.horn.row

# instFI = qubic.QubicInstrument(d)
# hornsFI = instFI.horn.open

# # hornsTD = (col >= 8) & (col <= 15) & (row >= 8) & (row <= 15)

# # instTD = qubic.QubicInstrument(d)
# # instTD.horn.open[~hornsTD] = False

# cnt = 1
# TDhornsFIconf = np.zeros(400)

# # %matplotlib notebook
# plt.figure(figsize=(10,10))
# q.horn.plot()
# for i in range(len(centers)):
#     #if hornsTD[i] == True:
#         #plt.text(centers[i,0]-0.006, centers[i,1], 'c{0:}'.format(col[i]), color='r',fontsize=8)
#         #plt.text(centers[i,0]+0.00001, centers[i,1], 'r{0:}'.format(row[i]), color='b',fontsize=8)
#     #plt.text(centers[i,0]-0.005, centers[i,1], '{0:.1f}'.format(centers[i,0]*1000), color='r',fontsize=8)
#     #plt.text(centers[i,0]-0.005, centers[i,1]-0.004, '{0:.1f}'.format(centers[i,1]*1000), color='r',fontsize=8)
#     plt.text(centers[i,0]-0.005, centers[i,1]-0.002, '{0:}'.format(str(i+1)), color='g',fontsize=9)
#     #plt.text(centers[i,0]-0.005, centers[i,1], 'td {0:}'.format(str(cnt)), color='r',fontsize=8)
    
#     if i+1 in [190,113]:
#         plt.plot(centers[i,0], centers[i,1], 'ro', markersize=20, alpha=0.5)

#     TDhornsFIconf[i] = cnt

#     cnt+=1
# #instTD.horn.plot()
# plt.ylabel('Horn GRF Y (m)')
# plt.xlabel('Horn GRF X (m)')

# plt.show()
# #plt.savefig("/home/james/OneDrive/Thesisv2/Figures/figsc35/hornlayout.png", bbox_inches='tight', facecolor='white')
```

### Let's define the coordinates of the horns

```{python}
# Which are the coordinates of these horns? In normalized units (unit= separation of two horns in one axis):

# Coordinates_horns = []

# count = 0
# for i in range(n):
#     for j in range(n):
#         a = (i,j)
#         print("Coordinates (x,y) are ", a)
        
#         Coordinates_horns.append(a)
        
#         count += 1

# print("count: ", count)

"""new for FI"""
Coordinates_horns = []

count = 0

for j in range(400):
    a = (q.horn.row[j], q.horn.column[j])
    print("Coordinates (x,y) are ", a)

    Coordinates_horns.append(a)

    count += 1

print("count: ", count)
```

```{python}
print(Coordinates_horns)
```

```{python}
Coordinates_horns = np.array(Coordinates_horns)
Coordinates_horns.shape
```

<!-- #region -->
**Note**:  The $i$ horn has coordinates $(x_i,y_i)$, which are stored in the $x_i*n +y_i$ element of the **Coordinates_horns** array.

For example, for the horn with coordinates $(2,3)$, the position in the array is:


$2n +3 = 19$

We can take this number as the label of the horn.

<!-- #endregion -->

### Now, let's compute the baselines


The number of baselines that you can construct with $N$ horns is ${N(N-1)} \over{2}$.

(This gives all posible combinations of two **different** horns, without repetition).

If we think of a $N \times N$ matrix with all the possible combinations, we would only take the upper (or lower) triangle.

```{python}
N_baselines = N*(N-1)/2

print(N_baselines)
```

 Each baseline can be thought of as a pseudo-vector, with a given length $L$ and a given orientation $\alpha$ (i.e., an oriented segment). To be able to compute $L$ and $\alpha$, we need to know the position of the horns that form the baseline.

Let's label somehow the baselines, using the label of the horns that constitute them.

If a baseline is formed with horns $i$ and $j$, let's take the upper triangle. Then if $i$ labels the row and $j$ labels the column, we will have: $j > i$.

So we do a loop over $i$ values, from $0$ to $N-1$, and then a nested loop over $j$ from $i+1$ to $N-1$.

For each, I have a baseline. I compute the $L^2$ and the $\tan (\alpha)$

$L^2= (x_i - x_j)^2 + ( y_i - y_j)^2 $

$\tan (\alpha) = (y_j - y_i)/(x_j - x_i)$

```{python}
# Check how many combinations we have:
count = 0
for i in range(N):
    for j in range(i+1,N):
        count = count +1
        print(count)
```

```{python}
# Let's test the coordinates of the horns that form a given baseline.

for i in range(N):
    for j in range(i+1,N):
        print("for the horn", i," the coordinates are: ", Coordinates_horns[i])
        print("for the horn", j," the coordinates are: ", Coordinates_horns[j])
        
```

```{python}
# For each baseline, let us compute L2 and tan_alpha:

baseline = []   #array that will keep L2 and the angle

baseline_v2 = []    #array that will keep the label of the horns that form the baseline, L2 and the angle

for i in range(N):
    x_i,y_i = Coordinates_horns[i]

    for j in range(i+1,N):
        
        x_j,y_j = Coordinates_horns[j]        


        L2 = (x_i - x_j)**2 + (y_i - y_j)**2
        
        tan_alpha = (y_j - y_i)/(x_j - x_i)
        
        angle= np.arctan(tan_alpha)
        
        baseline.append([L2, angle])
        
        baseline_v2.append([i,j, L2, angle])
        

baseline = np.array(baseline)

baseline_v2 = np.array(baseline_v2)

```

```{python}

```

```{python}
print(q.horn.row, q.horn.column, type(q.horn.column))

# Which are the coordinates of these horns? In normalized units (unit= separation of two horns in one axis):

Coordinates_horns = []

count = 0

for j in range(400):
    a = (q.horn.row[j], q.horn.column[j])
    print("Coordinates (x,y) are ", a)

    Coordinates_horns.append(a)

    count += 1

print("count: ", count)
```

```{python}
"""this way did not work properly"""

# For each baseline, let us compute L2 and tan_alpha:

# baseline = []   #array that will keep L2 and the angle

# baseline_v2 = []    #array that will keep the label of the horns that form the baseline, L2 and the angle

# # for i in range(N):
# #     x_i,y_i = Coordinates_horns[i]

# #     for j in range(i+1,N):
        
# #         x_j,y_j = Coordinates_horns[j]        


# #         L2 = (x_i - x_j)**2 + (y_i - y_j)**2
        
# #         tan_alpha = (y_j - y_i)/(x_j - x_i)
        
# #         angle= np.arctan(tan_alpha)
        
# #         baseline.append([L2, angle])
        
# #         baseline_v2.append([i,j, L2, angle])

# for i in range(len(centers[:,0])):
    
    
    
#     for j in range(i+1, len(centers[:,0])):
#         #print(i+1, centers[i,0], centers[i,1], j, centers[j,0], centers[j,1])
#         L2 = np.sqrt( (centers[i,0]- centers[j,0])**2 + (centers[i,1] - centers[j,1])**2 )
#         tan_alpha = (centers[j,1] - centers[i,1])/(centers[j,0] - centers[i,0])
#         angle= np.arctan(tan_alpha)
#         #print(i, j, L2, angle)
        
#         baseline.append([L2, angle])

#         baseline_v2.append([int(i), int(j), L2, angle])
        

# baseline = np.array(baseline)

# baseline_v2 = np.array(baseline_v2)

```

```{python}
# I order following L2 and then following angle. Then, I will need to separate them in subgroups to count
# how many there are in a given category.

from operator import itemgetter

ordered_baselines_v2 = sorted(baseline_v2, key= itemgetter(2,3))


ordered_baselines_v2 = np.array(ordered_baselines_v2)
```

```{python}

```

```{python}

```

```{python}
# Check that we have an ordered array (and we have the explicit number of the horns)

print(ordered_baselines_v2, ordered_baselines_v2.shape)
```

```{python}
# Another check to see if I'm getting what I want:

count = 0
for i in range(N):

    for j in range(i+1,N):      
        
        
        print(i,j, ordered_baselines_v2[count])
        
        count += 1
        
        
        
```

<!-- #region -->
 ### Intermediate computing.
    
In the following, we do some computations, to separate the baselines in categories according to the value of $L2$ and **angle**.


This is inspired by the example in the next (commented) cell.
<!-- #endregion -->

```{python active="", eval=FALSE}
## useful example taken from:
# https://stackoverflow.com/questions/31863083/python-split-numpy-array-based-on-values-in-the-array

np.split(arr, np.where(np.diff(arr[:,1]))[0]+1)


```

```{python}
# These are the L2 values:
ordered_baselines_v2[:,2]
```

```{python}
# I compute where the L2 value changes:
np.diff(ordered_baselines_v2[:,2])
```

```{python}
len(np.diff(ordered_baselines_v2[:,2]))
```

```{python}
xx = np.diff(ordered_baselines_v2[:,2])
```

```{python}
np.where(xx)[0]
```

```{python}
# These are the indices where the L2 value changes 
np.where(xx)[0]+1
```

```{python}
# I split the array in the positions where L2 changes:

zz= np.split(ordered_baselines_v2, np.where(np.diff(ordered_baselines_v2[:,2]))[0]+1)

```

```{python}

```

```{python}
#Check:
np.shape(zz)
```

```{python}
# Now, for each distinct value of L2, I split the array in different values of the angle.

partitioned_baselines = []

for i in range(len(zz)):

    
    aa = zz[i]
    
    bb = np.split(aa, np.where(np.diff(aa[:,3]))[0]+1)

    bb = np.array(bb)
    
    partitioned_baselines.append(bb)
    

partitioned_baselines = np.array(partitioned_baselines)

```

```{python}
print(np.shape(partitioned_baselines))

print(len(partitioned_baselines), partitioned_baselines.shape)

```

```{python}
# for each value of L2, how many different values of the angle we have:

for i in range(len(partitioned_baselines)):
    print(len(partitioned_baselines[i]), partitioned_baselines[i])
    
    
```

In the following cell, I compute each unique baseline (characterized by a given value of $L^2$ and an given angle), and compute how many equivalent baselines there are in each category. 

If we want to make tests using equivalent baselines, we can read the corresponding horns' labels from the $0$ and $1$ elements of the **partitioned_baselines** array, and make the appropriate selection.

```{python}
#figure out l and angle
print(partitioned_baselines[0][:,0,:])
```

```{python}
# Now I compute the number of unique baselines, and for each of them, 
# I compute the number of equivalent baselines (i.e. how many elements there are in that category):

N_unique = 0
Ncs = 0

for i in range(len(partitioned_baselines)):

    n_angles  = len(partitioned_baselines[i])
    
    for j in range(n_angles):

        print("L and angle for this BL type", partitioned_baselines[i][j][0][2:4], partitioned_baselines[i][j][0][2:4].shape)
         
        print(" ")
        
        N_eq = len(partitioned_baselines[i][j])
        
        Nc = int(N_eq*(N_eq-1)/2)

        print("# equiv BLs 4 particular type & N combs: ", N_eq, Nc)
        #print(" Shape of equivalent baseline set", partitioned_baselines[i][j].shape)
        #print(" ")
        
        N_unique += 1
        Ncs += Nc

        
print("Number of unique baselines: ", N_unique)
print("Total number of redundant baselines of every type: ", Ncs)
        
    
    
```

```{python}
# print(partitioned_baselines[:][0][0][0][3])
# print(partitioned_baselines[:][0][0][:,0].shape)
import matplotlib as mpl
```

```{python}
"""okay lets do a plot"""
colors = plt.cm.jet(np.linspace(0, 1, 378))

font = {'family' : 'normal',
        'weight' : 'normal',
        'size'   : 20}
plt.rc('font', **font)
plt.figure(figsize=(16,10))

for i in range(len(partitioned_baselines)):

    n_angles  = len(partitioned_baselines[i])
    
    for j in range(n_angles):
        #print(len(partitioned_baselines[i][j][:,0]), partitioned_baselines[i][j][0][3], partitioned_baselines[i][j][0][2])
        plt.plot(partitioned_baselines[i][j][0][3], partitioned_baselines[i][j][0][2], '.', 
                 color=colors[len(partitioned_baselines[i][j][:,0])-1], markersize=20)

cmap = mpl.cm.jet
norm = mpl.colors.Normalize(vmin=0, vmax=378)

plt.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),
             cax=None, orientation='vertical', label='Number of Baselines', ticks=np.linspace(0,378,15))

plt.ylabel('Distance Between Horns')
plt.xlabel(r'Angular Orientation ($\theta$)')

#plt.savefig('/home/james/OneDrive/Thesisv4/Figures/figs_baselines/numbaselinesintype_v2.png', facecolor='white')

# plt.scatter(partitioned_baselines[:][0][0][0][3], partitioned_baselines[:][0][0][0][2], 
#             c=len(partitioned_baselines[:][0][0][:,0]), cmap='jet', marker='.')


```

```{python}
"""double check this plot too"""
typefiles = '/home/james/mylibs/multifrequency/baseline_files/FI_baselines/'
typefiles = glob.glob(typefiles+'*.txt')
typefiles.sort(key=lambda f: int(re.sub('\D', '', f)))
typefiles = typefiles[0:740]
L2 = np.array([])
theta = np.array([])
nbl = np.array([])

print(len(typefiles))

for i, file in enumerate(typefiles):
    typedat = np.loadtxt(file, delimiter = ',', skiprows=1)
    #print(typedat[:,0].shape, typedat[0,2], typedat[0,3])
    
    L2 = np.append(L2, typedat[0,2])
    theta = np.append(theta, typedat[0,3])
    nbl = np.append(nbl, typedat[:,0].shape)
# %matplotlib inline
font = {'family' : 'normal',
        'weight' : 'normal',
        'size'   : 20}
plt.rc('font', **font)

plt.figure(figsize=(16,10))

plt.scatter(theta, L2, c= nbl, cmap='jet')
plt.colorbar(label='Number of Baselines')
plt.ylabel('Distance Between Horns ($mm^2$)')
plt.xlabel(r'Angular Orientation ($\theta$)')
#print(nbl)
```

```{python}
font = {'family' : 'normal',
        'weight' : 'normal',
        'size'   : 20}
plt.rc('font', **font)

fig = plt.figure(figsize=(15,14))

ax = fig.add_subplot(projection='polar')
c = ax.scatter(theta, L2, c=nbl, s=50, cmap='jet', alpha=0.75)

ax.set_thetamin(-95)
ax.set_thetamax(+95)
#ax.set_rorigin(-2.5)
ax.set_theta_zero_location('W', offset=-90)
label_position=ax.get_rlabel_position()

ax.text(-90, 210,'Distance Between Horns ($mm^2$)',
       rotation=label_position-27,ha='center',va='center')

cmap = mpl.cm.jet
norm = mpl.colors.Normalize(vmin=0, vmax=378)
fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap='jet'),
             cax=None, orientation='vertical', label='Number of Baselines', ticks=np.linspace(0,378,15), 
             shrink=.6, pad=0.05)
plt.tight_layout()
ax.set_rscale('symlog')
#ax.legend()
#print(label_position)
#plt.savefig('/home/james/OneDrive/Thesisv4/Figures/figs_baselines/polarTDBLs.png', facecolor='white')

```

```{python}
"""for each one of claudias baseline types
save for a file
produce auxilliary file"""

claudiasbaselinerep = "/home/james/mylibs/multifrequency/baseline_files/FI_baselines/"
```

```{python}
"""do not save this time"""

# fmt = '%d', '%d', '%1.4f', '%1.4f'

# N_unique = 0

# for i in range(len(partitioned_baselines)):

#     n_angles  = len(partitioned_baselines[i])
    
#     for j in range(n_angles):

#         #print(partitioned_baselines[i][j])
         
#         #print(" ")
        
#         N_eq = len(partitioned_baselines[i][j])

#         #print(" Number of equivalent baselines for this particular baseline: ", N_eq)
#         #print(" Shape of equivalent baseline set", partitioned_baselines[i][j].shape)
#         #print(" ")
        
#         N_unique += 1
        
#         index1_partitioned_baselines = partitioned_baselines[i][j][:,0:2] + 1
#         index2_partitioned_baselines = np.hstack((index1_partitioned_baselines, partitioned_baselines[i][j][:,2:4]))
#         #print(index1_partitioned_baselines.shape, index2_partitioned_baselines)
#         file = open(claudiasbaselinerep+"CBtype_"+str(N_unique)+".txt", "w")
#         file.write("pairFIh1, pairFIh2, L2, angle"+'\n')
#         np.savetxt(file, index2_partitioned_baselines, delimiter=', ', fmt=fmt)
#         file.close()
```

```{python}
cbfiles = glob.glob(claudiasbaselinerep+'*.txt')
print(len(cbfiles))

# for file in cbfiles:
#     print(file)
#     dat = np.loadtxt(file, delimiter=',', skiprows=1)
#     #print(len(dat.shape))
    
#     if len(dat.shape) == 1:
#         print("break for baseline of 1")
#         continue
    
#     bi = np.linspace(0, len(dat[:,0])-1, len(dat[:,0]), dtype=int)
#     arsize = scipy.special.factorial(len(bi)) / ( scipy.special.factorial(2) * scipy.special.factorial(len(bi) - 2))
#     #print("file, dat, bi, arsize, ", file, dat.shape, bi, arsize)
#     comb = np.array(list(combinations(bi, 2)))
#     #print(comb[0])
    
#     for i in range(len(comb[:,0])):
#         print(i, comb[i], comb[i,0], comb[i,1], 
#               dat[comb[i,0], 0], dat[comb[i,0], 1], dat[comb[i,1], 0], dat[comb[i,1], 1])
```

```{python}
"""above relates TD unique combinations
now setup FI numeric conversion"""
FIhorns = np.linspace(1,400,400, dtype=int)

d = qubic.qubicdict.qubicDict()
d.read_from_file('/home/james/libraries/qubic/qubic/dicts/pipeline_demo.dict')
d['config'] = 'FI'
q = qubic.QubicInstrument(d)

centers = q.horn.center[:, 0:2]
col = q.horn.column
row = q.horn.row

instFI = qubic.QubicInstrument(d)
hornsFI = instFI.horn.open

# hornsTD = (col >= 8) & (col <= 15) & (row >= 8) & (row <= 15)

# instTD = qubic.QubicInstrument(d)
# instTD.horn.open[~hornsTD] = False

cnt = 1
TDhornsFIconf = np.zeros(400)

# %matplotlib notebook
plt.figure(figsize=(10,10))
q.horn.plot()
for i in range(len(centers)):
    #if hornsTD[i] == True:
        #plt.text(centers[i,0]-0.006, centers[i,1], 'c{0:}'.format(col[i]), color='r',fontsize=8)
        #plt.text(centers[i,0]+0.00001, centers[i,1], 'r{0:}'.format(row[i]), color='b',fontsize=8)
    plt.text(centers[i,0]-0.005, centers[i,1], '{0:.1f}, {0:.1f}'.format(centers[i,0]*1000, centers[i,1]*1000), color='r',fontsize=8)
    plt.text(centers[i,0]-0.005, centers[i,1]-0.004, 'h {0:}'.format(str(i+1)), color='g',fontsize=8)
    #plt.text(centers[i,0]-0.005, centers[i,1], 'td {0:}'.format(str(cnt)), color='r',fontsize=8)

    TDhornsFIconf[i] = cnt

    cnt+=1
#instTD.horn.plot()
plt.ylabel('Horn GRF Y (m)')
plt.xlabel('Horn GRF X (m)')

plt.show()
```

```{python}
"""now convert TD to FI and check"""
# for file in cbfiles:
#     print(file)
#     dat = np.loadtxt(file, delimiter=',', skiprows=1)
#     #print(len(dat.shape))
    
#     if len(dat.shape) == 1:
#         print("break for baseline of 1")
#         continue
    
#     bi = np.linspace(0, len(dat[:,0])-1, len(dat[:,0]), dtype=int)
#     arsize = scipy.special.factorial(len(bi)) / ( scipy.special.factorial(2) * scipy.special.factorial(len(bi) - 2))
#     #print("file, dat, bi, arsize, ", file, dat.shape, bi, arsize)
#     comb = np.array(list(combinations(bi, 2)))
#     print(comb.shape)
    
#     for i in range(len(comb[:,0])):
#         print(i, comb[i], comb[i,0], comb[i,1], 
#               dat[comb[i,0], 0], dat[comb[i,0], 1], dat[comb[i,1], 0], dat[comb[i,1], 1],
#              int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,0]),0])]),
#              int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,0]),1])]),
#              int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,1]),0])]),
#              int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,1]),1])]))
```

```{python}
basedir='/home/james/libraries/qubic/qubic'
#basedir = Qubic_DataDir(datafile='instrument.py', ) 
print('basedir : ', basedir)
dictfilename = basedir + '/dicts/global_source_oneDet.dict'
d = qubic.qubicdict.qubicDict()
#d.read_from_file('../qubic/qubic/dicts/global_source_oneDet.dict')
#change to moddded dictionary
d.read_from_file('/home/james/libraries/qubic/qubic/dicts/global_source_oneDet.dict')
d['config'] = 'FI'
q = qubic.QubicMultibandInstrument(d)

vtxs = q[0].detector.vertex
vtxcounter = np.zeros(992)
print("vertexes shape: ", vtxs.shape)



```

```{python}
def RMSEtested(v1,v2):
    return np.sqrt(np.mean((v1-v2)**2)), mean_squared_error(v1, v2, squared=False)

def RMSE(v1,v2):
    return np.sqrt(np.sum((v1-v2)**2)/len(v1))

def RMSEo(v1,v2):
    """simple RMSE calculation for two arrays, typically focal plane intensity"""
    return np.sqrt(np.mean((v1-v2)**2))

from sklearn.metrics import mean_squared_error

x, y, i1 = calculate_intensity_4_baseline((120, 146), my150dat)
x, y, i2 = calculate_intensity_4_baseline((121, 147), my150dat)
#just to check...
MSE = mean_squared_error(i1, i2, sample_weight=i1,squared=True)
myrmse = RMSE(i1,i2)
rmseo = RMSEo(i1,i2)
print(MSE, myrmse, rmseo)
```

```{python}
"""now calc RMSE for each baseline in combination
use in loop above"""
my150dat = '/media/james/DATA/GRASPdata/MyTabSourceFIModel/150GHz/MODALfiles/'
#also need vtxs

def RMSEcalc(file):

    #print(i)
    x, y, i1 = calculate_intensity_4_baseline((120, 146), my150dat)
    x, y, i2 = calculate_intensity_4_baseline((121, 147), my150dat)
    
    rmse = RMSE(i1,i2)
    print(rmse)
    #px, py, pi = IntegrateHornCombOnFP(i, np.array([x, y]), vtxs)
    #print(x.shape, i.shape, px.shape, pi.shape)
    return rmse

rmse = RMSEcalc(my150dat)
print(RMSE)
```

```{python}
"""time to calc 1 horn pair as FP baseline and then integrat on dets = 37 seconds \n
lets say 378 pair combs in baseline = 37 * 2 * 378 \n
approx 8 hours
for that baseline type

I should probably try this parrallel function again...!
"""


```

```{python}
# import multiprocessing
# from joblib import Parallel, delayed
# from tqdm import tqdm
# from timeit import default_timer as timer
# import os
# import re
# from random import randint, sample

num_cores=16
inputs=tqdm(cbfiles)
```

```{python}
# startm = timer()

# if __name__ == '__main__':
#         process=Parallel(n_jobs=num_cores)(delayed(RMSEcalc)(file) for file in inputs)
        
# endm = timer()
# print("time taken parrallel", endm - startm, "s")


```

```{python}
#print(len(process), len(cbfiles))

#cbfilesshort = cbfiles[:57]

#print(cbfilesshort)

# import os
# import re
# from random import randint, sample
cbfiles = glob.glob(claudiasbaselinerep+'*.txt')

cbfiles.sort(key=lambda f: int(re.sub('\D', '', f)))

# print(cbfiles[645:650])

# cbfilesshort = cbfiles[645:650]

print(cbfiles[0:2])

cbfilesshort = cbfiles[0:2]

# print(cbfilesshort[90:100])

# file = open(claudiasbaselinerep+"RMSEs/"+"CBtypeRMSE_"+str(N_unique)+".txt", "w")
# file.write("h1, h2, h3, h4, FPRMSE, pixRMSE"+'\n')
# np.savetxt(file, index2_partitioned_baselines, delimiter=', ', fmt=fmt)
# file.close()

#16 files started approx 17:30 
#for all combs, 2 fps, 2 pix calcs
```

```{python}
#test random
#from random import randint, sample

x = [randint(0, 14) for p in range(0, 13)]
print(x)

#problem is numbers can repeat...
rs = sample(range(1, 14), 13)
print(rs)
```

```{python}
redupath = '/home/james/mylibs/multifrequency/baseline_files/FI_baselines/RMSEsreduced/'
my150dat = '/media/james/DATA/GRASPdata/MyTabSourceFIModel/150GHz/MODALfiles/'
def solve_quad(a, b, c):

    q1 = ( -b + np.sqrt(b**2 - 4*a*c) ) / 2*a
    q2 = ( -b + np.sqrt(b**2 - 4*a*c) ) / 2*a

    return q1,  q2

def reduce_combinations(file):
    #load file
    dat = np.loadtxt(file, delimiter=',', skiprows=1)
    #print(dat.shape)
    
    #return if no combinations
    if len(dat.shape) == 1:
        print("break for baseline of 1")
        return
    
    if len(dat) > 15:
        #x = [randint(0, len(dat[:,0])-1) for p in range(0, 15)]
        rs = sample(range(1, len(dat[:,0])), 15)
        #print(rs)

        #find combinations for reduced list
        comb = np.array(list(combinations(rs, 2)))
        print("> 15 ", rs, comb.shape)
        
    if len(dat) <= 15:
        #find combinations
        bi = np.linspace(0, len(dat[:,0])-1, len(dat[:,0]), dtype=int)
        comb = np.array(list(combinations(bi, 2)))
        print("<= 15 ", len(dat[:,0]), bi.shape, comb.shape)
        
        
    #open a file here and file orginal file
    fname = os.path.basename(file)  
    sfile = open(redupath+'RMSE_'+fname, "w")
    sfile.write("Orginal CB type file -> "+file+'\n')
    sfile.write("h1, h2, h3, h4, rmse"+'\n')
    
    for i in range(len(comb[:,0])):
#         print(i, comb[i], comb[i,0], comb[i,1], 
#               dat[comb[i,0], 0], dat[comb[i,0], 1], dat[comb[i,1], 0], dat[comb[i,1], 1],
#              int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,0]),0])]),
#              int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,0]),1])]),
#              int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,1]),0])]),
#              int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,1]),1])]))
    
    #print(i)
        x, y, i1 = calculate_intensity_4_baseline([int(dat[int(comb[i,0]),0]), int(dat[int(comb[i,0]),1])], my150dat)
        x, y, i2 = calculate_intensity_4_baseline([int(dat[int(comb[i,1]),0]), int(dat[int(comb[i,1]),1])], my150dat)
        rmse1 = RMSE(i1/max(i1), i2/max(i2))
#         px, py, pi1 = IntegrateHornCombOnFP(i1, np.array([x, y]), vtxs)
#         px, py, pi2 = IntegrateHornCombOnFP(i2, np.array([x, y]), vtxs)
#         rmse2 = RMSE(pi1, pi2)
        sfile.write("{}, {}, {}, {}, {:.5e} \n".format(
            int(dat[comb[i,0], 0]),
            int(dat[comb[i,0], 1]),
            int(dat[comb[i,1], 0]),
            int(dat[comb[i,1], 1]),
            rmse1))
        
    #close file here
    sfile.close()

    return


```

```{python}
"""debug and test funvtion here"""

startm = timer()

for f in cbfilesshort:
    reduce_combinations(f)

endm = timer()
print("time taken parrallel", endm - startm, "s")

"""5 files less than 3 mins
approx 0.6 min per file
774 files gives 464 mins or 7.7 hours
but really should be less since not all files will match the condition...

324 s for 5 files is 5.4 mins for 5 is 1.1 min for 1"""
```

```{python}
num_cores=15
inputs=tqdm(cbfilesshort)
inputs=tqdm(cbfiles)

startm = timer()

if __name__ == '__main__':
        process=Parallel(n_jobs=num_cores)(delayed(reduce_combinations)(file) for file in inputs)
        
endm = timer()
print("time taken parrallel", endm - startm, "s")

"""89 s for 5 which should also be 89 s for 15/16
774/15 = 52
52 * 1.5mins = 77 mins

total actual time was 8572 s = 143m = 2.4 hrs"""
```

```{python}
"""try replicate error for type 637"""
#print(cbfiles[636])
def reduce_combinations_debug(file):
    #load file
    dat = np.loadtxt(file, delimiter=',', skiprows=1)
    #print(dat.shape)
    
    #return if no combinations
    if len(dat.shape) == 1:
        print("break for baseline of 1")
        return
    
    if len(dat) > 15:
        #x = [randint(0, len(dat[:,0])-1) for p in range(0, 15)]
        rs = sample(range(1, len(dat[:,0])), 15)
        #print(rs)

        #find combinations for reduced list
        comb = np.array(list(combinations(rs, 2)))
        print("> 15 ", rs, comb.shape, os.path.basename(file))
        
    if len(dat) <= 15:
        #find combinations
        bi = np.linspace(0, len(dat[:,0])-1, len(dat[:,0]), dtype=int)
        comb = np.array(list(combinations(bi, 2)))
        print("<= 15 ", len(dat[:,0]), bi.shape, comb.shape, os.path.basename(file))
        
        
#     #open a file here and file orginal file
#     fname = os.path.basename(file)  
#     sfile = open(redupath+'RMSE_'+fname, "w")
#     sfile.write("Orginal CB type file -> "+file+'\n')
#     sfile.write("h1, h2, h3, h4, rmse"+'\n')
    
#     for i in range(len(comb[:,0])):
# #         print(i, comb[i], comb[i,0], comb[i,1], 
# #               dat[comb[i,0], 0], dat[comb[i,0], 1], dat[comb[i,1], 0], dat[comb[i,1], 1],
# #              int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,0]),0])]),
# #              int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,0]),1])]),
# #              int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,1]),0])]),
# #              int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,1]),1])]))
    
#     #print(i)
#         x, y, i1 = calculate_intensity_4_baseline([int(dat[int(comb[i,0]),0]), int(dat[int(comb[i,0]),1])], my150dat)
#         x, y, i2 = calculate_intensity_4_baseline([int(dat[int(comb[i,1]),0]), int(dat[int(comb[i,1]),1])], my150dat)
#         rmse1 = RMSE(i1/max(i1), i2/max(i2))
# #         px, py, pi1 = IntegrateHornCombOnFP(i1, np.array([x, y]), vtxs)
# #         px, py, pi2 = IntegrateHornCombOnFP(i2, np.array([x, y]), vtxs)
# #         rmse2 = RMSE(pi1, pi2)
#         sfile.write("{}, {}, {}, {}, {:.5e} \n".format(
#             int(dat[comb[i,0], 0]),
#             int(dat[comb[i,0], 1]),
#             int(dat[comb[i,1], 0]),
#             int(dat[comb[i,1], 1]),
#             rmse1))
        
#     #close file here
#     sfile.close()

    return

startm = timer()

for f in cbfiles:
    reduce_combinations_debug(f)
```

```{python}
import os
#os.path.basename('/folderA/folderB/folderC/folderD')
rmsepath = '/home/james/mylibs/multifrequency/baseline_files/FI_baselines/RMSEs/'
rmseApath = '/home/james/mylibs/multifrequency/baseline_files/FI_baselines/RMSEs/analysisdata/'

def RMSEcalc(file):
    #load file
    dat = np.loadtxt(file, delimiter=',', skiprows=1)
    
    if len(dat.shape) == 1:
        print("break for baseline of 1")
        return

    bi = np.linspace(0, len(dat[:,0])-1, len(dat[:,0]), dtype=int)
    comb = np.array(list(combinations(bi, 2)))
    
    #open a file here and file orginal file
    fname = os.path.basename(file)  
    sfile = open(rmsepath+'RMSE_'+fname, "w")
    sfile.write("Orginal CB type file -> "+file+'\n')
    sfile.write("tdh1, tdh2, tdh3, tdh4, h1, h2, h3, h4, rmse"+'\n')
    
    for i in range(len(comb[:,0])):
        print(i, comb[i], comb[i,0], comb[i,1], 
              dat[comb[i,0], 0], dat[comb[i,0], 1], dat[comb[i,1], 0], dat[comb[i,1], 1],
             int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,0]),0])]),
             int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,0]),1])]),
             int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,1]),0])]),
             int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,1]),1])]))
    
    #print(i)
        x, y, i1 = calculate_intensity_4_baseline((int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,0]),0])]), int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,0]),1])])), my150dat)
        x, y, i2 = calculate_intensity_4_baseline((int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,1]),0])]), int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,1]),1])])), my150dat)
        rmse1 = RMSE(i1/max(i1), i2/max(i2))
#         px, py, pi1 = IntegrateHornCombOnFP(i1, np.array([x, y]), vtxs)
#         px, py, pi2 = IntegrateHornCombOnFP(i2, np.array([x, y]), vtxs)
#         rmse2 = RMSE(pi1, pi2)
        sfile.write("{}, {}, {}, {}, {}, {}, {}, {}, {:.5e} \n".format(
            int(dat[comb[i,0], 0]),
            int(dat[comb[i,0], 1]),
            int(dat[comb[i,1], 0]),
            int(dat[comb[i,1], 1]),
            int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,0]),0])]),
            int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,0]),1])]),
            int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,1]),0])]),
            int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,1]),1])]),
            rmse1))
        
    #close file here
    sfile.close()
    
    return 

def RMSEcalcFI(file):
    #load file
    dat = np.loadtxt(file, delimiter=',', skiprows=1)
    
    if len(dat.shape) == 1:
        print("break for baseline of 1")
        return

    bi = np.linspace(0, len(dat[:,0])-1, len(dat[:,0]), dtype=int)
    comb = np.array(list(combinations(bi, 2)))
    
    #open a file here and file orginal file
    fname = os.path.basename(file)  
    sfile = open(rmsepath+'RMSE_'+fname, "w")
    sfile.write("Orginal CB type file -> "+file+'\n')
    sfile.write("h1, h2, h3, h4, rmse"+'\n')
    
    for i in range(len(comb[:,0])):
#         print(i, comb[i], comb[i,0], comb[i,1], 
#               dat[comb[i,0], 0], dat[comb[i,0], 1], dat[comb[i,1], 0], dat[comb[i,1], 1],
#              int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,0]),0])]),
#              int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,0]),1])]),
#              int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,1]),0])]),
#              int(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,1]),1])]))
    
    #print(i)
        x, y, i1 = calculate_intensity_4_baseline([int(dat[int(comb[i,0]),0]), int(dat[int(comb[i,0]),1])], my150dat)
        x, y, i2 = calculate_intensity_4_baseline([int(dat[int(comb[i,1]),0]), int(dat[int(comb[i,1]),1])], my150dat)
        rmse1 = RMSE(i1/max(i1), i2/max(i2))
#         px, py, pi1 = IntegrateHornCombOnFP(i1, np.array([x, y]), vtxs)
#         px, py, pi2 = IntegrateHornCombOnFP(i2, np.array([x, y]), vtxs)
#         rmse2 = RMSE(pi1, pi2)
        sfile.write("{}, {}, {}, {}, {:.5e} \n".format(
            int(dat[comb[i,0], 0]),
            int(dat[comb[i,0], 1]),
            int(dat[comb[i,1], 0]),
            int(dat[comb[i,1], 1]),
            rmse1))
        
    #close file here
    sfile.close()
    
    return


```

```{python}
# num_cores=16
# inputs=tqdm(cbfiles)

# startm = timer()

# if __name__ == '__main__':
#         process=Parallel(n_jobs=num_cores)(delayed(RMSEcalcFI)(file) for file in inputs)
        
# endm = timer()
# print("time taken parrallel", endm - startm, "s")
```

```{python}
"""95 - 108 10 seconds with no TES
85 - 108 24 seconds with no TES
85 - 108 with TES calc = 
55 mins for all combs and files with no TES rmse calc
"""
```

```{python}
"""here just do a quick test comparing FI and TD functions """


```

```{python}
redupath = '/home/james/mylibs/multifrequency/baseline_files/FI_baselines/RMSEsreduced/'
#rmsepath = '/home/james/mylibs/multifrequency/baseline_files/ClaudiasBaselines/RMSEs/'
typefiles = '/home/james/mylibs/multifrequency/baseline_files/FI_baselines/'

jet= plt.get_cmap('jet')
colors = iter(jet(np.linspace(0,10,11)))

rmsefiles = glob.glob(redupath+'*.txt')
rmsefiles.sort(key=lambda f: int(re.sub('\D', '', f)))
#print(rmsefiles)
typefiles = glob.glob(typefiles+'*.txt')
typefiles.sort(key=lambda f: int(re.sub('\D', '', f)))

typelist = np.array([])
rmses = np.array([])
rmsesm = np.array([])
rmsesSTD = np.array([])
ind = np.array([])
L2 = np.array([])
theta = np.array([])

font = {'family' : 'normal',
        'weight' : 'normal',
        'size'   : 20}
plt.rc('font', **font)
# %matplotlib inline
plt.rc('font', **font)
plt.figure(figsize=(16, 8))

temp=0
#740 since only one baseline for these types
for i, file in enumerate(rmsefiles[0:740]):
    
    typedat = np.loadtxt(typefiles[i], delimiter = ',', skiprows=1, max_rows=1)
    
    num = re.findall(r'\d+', file)
    #print(file, num, i)
    
    dat = np.loadtxt(file, delimiter = ',', skiprows=2)
    
    temp = len(dat[:,4].shape) + temp
    
    #index = index+temp
        
    nums = np.full(dat[:,4].shape, num)
    
    #print(dat.shape, i, dat[:,8].shape, nums.shape)
    
    
    rmses = np.append(rmses, dat[:,4])
    rmsesm = np.append(rmsesm, np.mean(dat[:,4]))
    rmsesSTD = np.append(rmsesSTD, np.std(dat[:,4]))
    typelist = np.append(typelist, nums)
    #ind = np.append(ind, index)
    L2 = np.append(L2, typedat[2])
    theta = np.append(theta, typedat[3])
    
    plt.plot(nums, dat[:,4], '.', markersize=10)

plt.legend(loc='upper right', fontsize=12)
plt.xlabel('Baseline Type')
plt.ylabel('RMSE')

#plt.savefig('/media/james/DATA/baseline_figures/results_cb_all_FI.png', facecolor='white')

plt.figure(figsize=(16, 8))
for i, file in enumerate(rmsefiles[0:740]):
    plt.plot(i+1, rmsesm[i], 's', markersize=5)
    
plt.gca().set_prop_cycle(None)
for i, file in enumerate(rmsefiles[0:740]):
    plt.errorbar(i+1, rmsesm[i], yerr=rmsesSTD[i], elinewidth=3)
    
plt.xlabel('Baseline Type')
plt.ylabel('Mean RMSE with Standard Deviation')

#plt.xlim([0,100])

#plt.savefig('/media/james/DATA/baseline_figures/results_cb_meanstd_FI_full.png', facecolor='white')
font = {'family' : 'normal',
        'weight' : 'normal',
        'size'   : 20}
plt.rc('font', **font)
plt.figure(figsize=(16,10))

plt.scatter(theta[::-1], L2[::-1], c= rmsesm[::-1], cmap='jet', s=65)
plt.colorbar(label='Mean RMSE of Baseline Type')
plt.ylabel('Distance Between Horns')
plt.xlabel(r'Angular Orientation ($\theta$)')
#plt.savefig('/home/james/OneDrive/Thesisv4/Figures/figs_baselines/rmseofBLtype_v2.png', facecolor='white')


```

```{python}
font = {'family' : 'normal',
        'weight' : 'normal',
        'size'   : 20}
plt.rc('font', **font)

fig = plt.figure(figsize=(15,16))

ax = fig.add_subplot(projection='polar')
c = ax.scatter(theta[::-1], L2[::-1], c= rmsesm[::-1], cmap='jet', s=65)

ax.set_thetamin(-95)
ax.set_thetamax(+95)
#ax.set_rorigin(-2.5)
ax.set_theta_zero_location('W', offset=-90)
label_position=ax.get_rlabel_position()

ax.text(-90, 2,'Distance Between Horns ($mm^2$)',
       rotation=label_position-27,ha='center',va='center')

cmap = mpl.cm.jet
norm = mpl.colors.Normalize(vmin=0, vmax=max(rmsesm))
fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap='jet'),
             cax=None, orientation='vertical', label='Mean RMSE of Baseline Type', ticks=np.linspace(0,0.2,9), 
             shrink=.5, pad=0.05)
plt.tight_layout()
ax.set_rscale('symlog')
#ax.legend()
#print(label_position)
#plt.savefig('/home/james/OneDrive/Thesisv4/Figures/figs_baselines/polarFI.png', facecolor='white')
```

```{python}
print(rmsesm.shape, partitioned_baselines.shape)
colors = plt.cm.jet(np.linspace(0, 1, 378))
print(len(partitioned_baselines[0][0][:,0])-1, colors[len(partitioned_baselines[0][0][:,0])-1])
colors = plt.cm.jet(np.linspace(0, max(rmsesm), len(rmsesm)))
print(rmsesm[0], colors.shape, colors[np.where(rmsesm[0])])

print(colors)

rmsesmNORM = rmsesm*1000


colors = plt.cm.jet(np.linspace(0, 1, len(rmsesmNORM)))

print(int(rmsesmNORM[0]), colors[int(rmsesmNORM[0])])
print(colors)
```

```{python}
"""okay lets do a plot *** this method NOT GOODl"""
colors = plt.cm.jet(np.linspace(0, 1, max(rmsesmNORM)))

font = {'family' : 'normal',
        'weight' : 'normal',
        'size'   : 20}
plt.rc('font', **font)
plt.figure(figsize=(16,10))
t=0

for i in range(len(partitioned_baselines)):

    n_angles  = len(partitioned_baselines[i])
    
    for j in range(n_angles):
        #print(len(partitioned_baselines[i][j][:,0]), rmsesm[i], colors[np.where(rmsesm[i])])
        #print(colors[int(rmsesmNORM[i])])
        plt.plot(partitioned_baselines[i][j][0][3], partitioned_baselines[i][j][0][2], '.', 
                 color=colors[int(rmsesmNORM[i])], markersize=20)
        t+=1
cmap = mpl.cm.jet
norm = mpl.colors.Normalize(vmin=0, vmax=max(rmsesm))

plt.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),
             cax=None, orientation='vertical', label='Mean RMSE of Baseline Type')

plt.ylabel('Distance Between Horns (mm)')
plt.xlabel(r'Angular Orientation ($\theta$)')
#plt.savefig('/home/james/OneDrive/Thesisv4/Figures/figs_baselines/rmseofBLtype.png', facecolor='white')
#print(t, colors.shape)
```

```{python}
print(len(typelist), len(rmses), len(ind))


```

```{python}
from matplotlib.pyplot import cm
import seaborn as sns
color=iter(cm.rainbow(np.linspace(0,len(typelist),len(typelist))))
colors = iter(plt.cm.Spectral(np.linspace(0,len(typelist),len(typelist))))
#print(colors.shape)

colors = sns.palplot(sns.color_palette("hls", len(typelist)))

color=iter(cm.rainbow(np.linspace(0,1,len(typelist))))
# for i in range(n):
#    cn = next(color)
#    plt.plot(x, y,c=c)


plt.figure(figsize=(16, 8))

plt.scatter(typelist, rmses, '.', color=colors)

#for i in range(len(typelist)):
    

# # # #     plt.plot(typelist[i], rmses[i], '.', 
# # # #              color=(0, typelist[i] / max(typelist), typelist[i] / max(typelist), typelist[i] / max(typelist)))
#     plt.plot(typelist[i], rmses[i], color=c)

#     if typelist[i] > typelist[i-1]:
#         c=next(color)
    
```

```{python}
# startm = timer()
# ls = []
# for file in cbfiles:
#     ls.append(RMSEcalc(file))

# endm = timer()
# print("time taken cpu loop", endm - startm, "s")
```

```{python}
"""okay great so parrallel is faster this time!"""
```

```{python}
# def RMSEcalc2(horns):
#     #print(horns)

#     #print(i)
#     x, y, i1 = calculate_intensity_4_baseline((121, 147), my150dat)
#     x, y, i2 = calculate_intensity_4_baseline((120, 146), my150dat)
#     rmse = RMSE(i1,i2)
#     print(rmse)
#     #px, py, pi = IntegrateHornCombOnFP(i, np.array([x, y]), vtxs)
#     #print(x.shape, i.shape, px.shape, pi.shape)
#     return rmse
```

```{python}
# """now try with file as main loop but combs as parrallel"""

# """now convert TD to FI and check 50, 51 has a problem type 97"""
# for file in cbfiles[0:1]:
#     print(file)
#     dat = np.loadtxt(file, delimiter=',', skiprows=1)
#     #print(len(dat.shape))
    
#     if len(dat.shape) == 1:
#         print("break for baseline of 1")
#         continue
    
#     bi = np.linspace(0, len(dat[:,0])-1, len(dat[:,0]), dtype=int)
#     arsize = scipy.special.factorial(len(bi)) / ( scipy.special.factorial(2) * scipy.special.factorial(len(bi) - 2))
#     #print("file, dat, bi, arsize, ", file, dat.shape, bi, arsize)
#     comb = np.array(list(combinations(bi, 2)))
#     print(comb.shape)
    
#     h1 = []
#     h2 = []
#     h3 = []
#     h4 = []
    
#     for i in range(len(comb[:,0])):
    
#         h1.append(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,0]),0])])
#         h2.append(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,0]),1])])
#         h3.append(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,1]),0])])
#         h4.append(FIhorns[np.where(TDhornsFIconf == dat[int(comb[i,1]),1])])
    
#     ha = np.hstack([h1, h2, h3, h4])
#     print(ha.shape)

# #     num_cores = 16

# #     if len(ha[:,0]) <= 16:
# #         num_cores=1
# #         continue
#     inputs=tqdm(ha)
    
    
#     if __name__ == '__main__':
#         process=Parallel(n_jobs=num_cores)(delayed(RMSEcalc2)(i) for i in inputs)
        
#     #RMSE for these combinations
#     RMSE = np.array(process)
#     print("RMSE shape, ", RMSE.shape)
    
#     tosave = np.hstack([ha, RMSE])
#     print("to save shape,  ", tosave.shape)
```

```{python}
# #print(len(inputs))
# print(ha.shape)
```

```{python}
# for i in ha:
#     print(i)
```

```{python}
# inputs=tqdm(ha)

# startm = timer()
    
# if __name__ == '__main__':
#     proces=Parallel(n_jobs=num_cores)(delayed(RMSEcalc2)(i) for i in inputs)

        
# endm = timer()
# print("time taken parrallel", endm - startm, "s")
```

```{python}
# print(len(proces), proces)
```

```{python}
"""time to loop over 378 horn combs a calc rmse residual approx 32 seconds"""
```

```{python}
# inputs=tqdm(ha)
```

```{python}
# print(inputs)
```

```{python}
def RMSEcalc3(h1, h2, h3, h4):
    #print(horns)

    #print(i)
    x, y, i1 = calculate_intensity_4_baseline((h1, h2), my150dat)
    x, y, i2 = calculate_intensity_4_baseline((h3, h4), my150dat)
    rmse1 = RMSE(i1, i2)
    print(rmse)
    px, py, pi1 = IntegrateHornCombOnFP(i, np.array([x, y]), vtxs)
    px, py, pi2 = IntegrateHornCombOnFP(i, np.array([x, y]), vtxs)
    rmse2 = RMSE(pi1, pi2)
    #print(x.shape, i.shape, px.shape, pi.shape)
    return rmse1, rmse2
```

```{python}
def RMSEforHornCombs(file):
    print(file)
    #load file
    dat = np.loadtxt(file, delimiter=',', skiprows=1)
    #print(len(dat.shape))
    
    #check to make sure combination exists
    if len(dat.shape) == 1:
        print("break for baseline of 1")
        return
        
    #create indexed combinations
    bi = np.linspace(0, len(dat[:,0])-1, len(dat[:,0]), dtype=int)
    #arsize = scipy.special.factorial(len(bi)) / ( scipy.special.factorial(2) * scipy.special.factorial(len(bi) - 2))
    #print("file, dat, bi, arsize, ", file, dat.shape, bi, arsize)
    comb = np.array(list(combinations(bi, 2)))
    print(comb.shape)
    
    h1 = np.array(len(comb[:,0]))
    h2 = np.array(len(comb[:,0]))
    h3 = np.array(len(comb[:,0]))
    h4 = np.array(len(comb[:,0]))
    #set TD horns in FI indexing
    h1 = np.array(FIhorns[np.where(TDhornsFIconf == dat[int(comb[:,0]),0])])
    h2 = FIhorns[np.where(TDhornsFIconf == dat[int(comb[:,0]),1])]
    h3 = FIhorns[np.where(TDhornsFIconf == dat[int(comb[:,1]),0])]
    h4 = FIhorns[np.where(TDhornsFIconf == dat[int(comb[:,1]),1])]
    
    #calculation RMSE for each horn combination
    RMSE_FP = []
    RMSE_dets = []
    for i in range(len(comb[:,0])):
    
        rmse1, rmse2 = RMSEcalc3(h1[i], h2[i], h3[i], h4[i])
        
        RMSE_FP.append(rmse1)
        RMSE_dets.append(rmse2)
        
    RMSE_FP = np.array(RMSE_FP)
    RMSE_dets = np.array(RMSE_dets)
    
    savear = np.hstack([h1, h2, h3, h4, RMSE_FP, RMSE_dets])
    print(savear.shape)
    
    #now save to file
    
    return
```

```{python}
num_cores=16
inputs=tqdm(cbfiles)

startm = timer()

if __name__ == '__main__':
        process=Parallel(n_jobs=num_cores)(delayed(RMSEforHornCombs)(file) for file in inputs)
        
endm = timer()
print("time taken parrallel", endm - startm, "s")
```

```{python}

```
